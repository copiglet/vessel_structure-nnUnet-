{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "# import dicom2nifti\n",
    "from skimage.measure import label, regionprops\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import nibabel\n",
    "# from pydicom.uid import ExplicitVRLittleEndian\n",
    "import numpy as np\n",
    "import sys\n",
    "# import dicom2nifti.settings as settings\n",
    "import SimpleITK as sitk\n",
    "# import pydicom\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "def write_dataset_descp(main_dir, task_name, task_id):\n",
    "    task_folder_name = os.path.join(main_dir, 'nnUNet_raw', 'Task' + task_id + '_' + task_name)\n",
    "    overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Task_folder\n",
    "    json_file_exist = False\n",
    "\n",
    "    if os.path.exists(os.path.join(task_folder_name,'dataset.json')):\n",
    "        print('dataset.json already exist!')\n",
    "        json_file_exist = True\n",
    "\n",
    "    if json_file_exist==False or overwrite_json_file:\n",
    "\n",
    "        json_dict = OrderedDict()\n",
    "        json_dict['name'] = task_name\n",
    "        json_dict['description'] = \"Vessel Detection\"\n",
    "        json_dict['tensorImageSize'] = \"3D\"\n",
    "        json_dict['reference'] = \"see challenge website\"\n",
    "        json_dict['licence'] = \"see challenge website\"\n",
    "        json_dict['release'] = \"0.0\"\n",
    "\n",
    "        #you may mention more than one modality\n",
    "        json_dict['modality'] = {\n",
    "            \"0\": \"MRI\"\n",
    "        }\n",
    "        #labels+1 should be mentioned for all the labels in the dataset\n",
    "        json_dict['labels'] = {\n",
    "            \"0\": \"background\",\n",
    "            \"1\": \"Vessel\"\n",
    "        }\n",
    "\n",
    "        train_ids = os.listdir(os.path.join(task_folder_name, 'labelsTr'))\n",
    "        test_ids = os.listdir(os.path.join(task_folder_name, 'imagesTs'))\n",
    "\n",
    "        json_dict['numTraining'] = len(train_ids)\n",
    "        json_dict['numTest'] = len(test_ids)\n",
    "\n",
    "        #no modality in train image and labels in dataset.json\n",
    "        json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "        print(json_dict['training'])\n",
    "\n",
    "        #removing the modality from test image name to be saved in dataset.json\n",
    "        json_dict['test'] = [\"./imagesTs/%s\" % (i[:i.find(\"_0000\")]+'.nii.gz') for i in test_ids]\n",
    "        print(json_dict['test'])\n",
    "\n",
    "        with open(os.path.join(task_folder_name,\"dataset.json\"), 'w') as f:\n",
    "            json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(task_folder_name,'dataset.json')):\n",
    "            if json_file_exist==False:\n",
    "                print('dataset.json created!')\n",
    "            else:\n",
    "                print('dataset.json overwritten!')\n",
    "\n",
    "\n",
    "\n",
    "root = '/home/dante/nnUNet'\n",
    "\n",
    "\n",
    "os.environ['nnUNet_raw'] = os.path.join(root,'nnUNet_raw')\n",
    "os.environ['nnUNet_preprocessed'] = os.path.join(root,'preprocessed')\n",
    "os.environ['RESULTS_FOLDER'] = os.path.join(root,'RESULTS_FOLDER')\n",
    "\n",
    "\n",
    "\n",
    "write_dataset_descp(root, 'Vassels', '521')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset_descp(root, 'TARGET', '100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting old version to new version\n",
    "\n",
    "os.system('nnUNetv2_convert_old_nnUNet_dataset Task521_Vassels Dataset521_Vassels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "os.system('nnUNetv2_plan_and_preprocess -d 521 --verify_dataset_integrity -np 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "os.system('nnUNetv2_train 521 3d_fullres 0 --npz ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "\n",
    "os.system('nnUNetv2_predict -i input -o output -d Dataset521_Vassels -c 3d_fullres -f 0')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
